{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#\t\t\t\t\tWritten by: Yih Kai Teh\t\t\t\t     \t\t\t    \t\t\t\t\t\t\t\t\t\t#\n",
    "#\t\t\t\t\t\t\t\t\t\t\t\t\t\t    \t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
    "#  \tThis project is from one of my modules (Advanced Topic in Machine Learning) at UCL taught by Google DeepMind   \t#\n",
    "#\t\t\t\t\t\t\t\t\t\t\t\t\t\t    \t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
    "#\t\t\t\tThis file is only for evaluation, not for training\t\t\t\t    \t\t\t\t\t\t\t\t#\n",
    "#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
    "#\t\t\t\t\tEdited by: Francesco Stranieri\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t    #\n",
    "#\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\n",
    "#####################################################################################################################\n",
    "\n",
    "# Due to the limited time when running this games, the input height and width are further reduced into 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import convolution2d, fully_connected\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = \"MsPacman\"\n",
    "version = \"-v4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(game+version)\t\t\t\t\t\t# atari games selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000000 / 4\t\t\t\t\t\t\t\t# train for 1 millions steps (can increase for more if have better GPU)\n",
    "batch_size = 64\t\t\t\t\t\t\t\t\t\t# mini batch size used for optimization\n",
    "discount_rate = 0.99\t\t\t\t\t\t\t\t# discount rate (control the value of reward for near future or distant future)\n",
    "test_episode = int(100/2)\t\t\t\t\t\t\t# number of test episodes to run for evaluation as each episode is stostatic\n",
    "\n",
    "input_height = input_width = 28\t\t\t\t\t\t# size of the reduced image height and width \n",
    "input_channels = 4\t\t\t\t\t\t\t\t\t# the number of frame to stack together in order to capture the motion\n",
    "\n",
    "conv_kernel_output_channel = [16, 32]\t\t\t\t# output channel of kernel/filter for CNN\n",
    "conv_kernel_sizes = [(6,6), (4,4)]\t\t\t\t\t# size of the kernel/filter for CNN\n",
    "conv_strides = [2, 2]\t\t\t\t\t\t\t\t# number of strides for the kernel/filter to slide across image\n",
    "conv_paddings = [\"SAME\"] * 2\t\t\t\t\t\t# padding choice \n",
    "conv_activation = [tf.nn.relu] * 2\t\t\t\t\t# activation for CNN (RELU is used here)\n",
    "\n",
    "n_hidden_inputs = input_height * input_width * 2\t# size of the flatten layer\n",
    "n_hidden = 256\t\t\t\t\t\t\t\t\t\t# number of hidden layer\n",
    "n_outputs = env.action_space.n\t\t\t\t\t\t# number of possible output for the game (this is different for each game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.random_normal_initializer(seed=600, stddev=0.01)\t# initializer for weight\n",
    "b_initializer = tf.constant_initializer(0.1)\t\t\t\t\t\t# initializer for bias\n",
    "learning_rate = 0.0001\t\t\t\t\t\t\t\t\t\t\t\t# learning rate for the optimizater\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q network architecture (conv(6x6x16) -> RELU -> conv(4x4x32) -> RELU -> flatten -> hidden layer (256 units) -> RELU -> output layer(number of actions))\n",
    "def q_network(X_state, scope):\n",
    "\tprev_layer = X_state\n",
    "\twith tf.variable_scope(scope) as scope:\n",
    "\t\tfor n_maps, kernel_size, stride, padding, activation in zip(conv_kernel_output_channel, conv_kernel_sizes, conv_strides, conv_paddings, conv_activation):\n",
    "\t\t\tprev_layer = convolution2d(prev_layer, num_outputs=n_maps, kernel_size=kernel_size, stride=stride, padding=padding, activation_fn=activation, weights_initializer=initializer, biases_initializer=b_initializer)\n",
    "\t\tlast_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_inputs])\n",
    "\t\thidden = fully_connected(last_conv_layer_flat, n_hidden, activation_fn=tf.nn.relu, weights_initializer=initializer, biases_initializer=b_initializer)\n",
    "\t\toutputs = fully_connected(hidden, n_outputs, activation_fn=None, weights_initializer=initializer, biases_initializer=b_initializer)\n",
    "\ttrainable_vars = {var.name[len(scope.name):]: var for var in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope.name)}\n",
    "\treturn outputs, trainable_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width, input_channels])\t# placeholder for the state\n",
    "target_Q_value, stationary_target_vars = q_network(X_state, scope=\"target_q_networks\")\t\t\t# the Q value from the stationary network\n",
    "Q_value, current_target_vars = q_network(X_state, scope=\"q_networks\")\t\t\t\t\t\t\t# the Q value from the current network\n",
    "\n",
    "# copy operation to update the stationary network by copying over from current network every 5k steps\n",
    "copy = [stationary_target_var.assign(current_target_vars[var_name]) for var_name, stationary_target_var in stationary_target_vars.items()]\n",
    "update_stationary_target = tf.group(*copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"train\"):\n",
    "\tX_action = tf.placeholder(tf.int32, shape=[None])\t\t\t\t\t\t\t\t\t\t# placeholder for the action\n",
    "\ty = tf.placeholder(tf.float32, shape=[None])\t\t\t\t\t\t\t\t\t\t\t# placeholder for the true Q value\n",
    "\tqvalue = tf.reduce_sum(tf.multiply(Q_value, tf.one_hot(X_action, n_outputs)), axis=1)\t# calculate the Q value\n",
    "\tcost = tf.reduce_mean(tf.square(y - qvalue))\t\t\t\t\t\t\t\t\t\t\t# define the cost function (differences between the true and estimated Q value)\n",
    "\toptimizer = tf.train.AdamOptimizer(learning_rate)\t\t\t\t\t\t\t\t\t\t# optimizer used is ADAM, can be changed for other e.g. SGD\n",
    "\ttraining_op = optimizer.minimize(cost)\t\t\t\t\t\t\t\t\t\t\t\t\t# minimize the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess observation to convert RGB into greyscale, remove some extra useless blank on the side, and resize. (Mainly to speed up computation and improve performance)\n",
    "def preprocess_observation(obs):\n",
    "\timg = resize(rgb2gray(obs[:, :, :]), (input_height, input_width), mode='constant')\n",
    "\timg = np.reshape(img, [input_height, input_width, 1])\n",
    "\treturn img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up the initial 4 frames in each eipsodes\n",
    "def initial_4_frames():\n",
    "\tfor t in range(input_channels):\n",
    "\t\tif t == 0:\t\t\t\t\t\t\t\t# for the first frame\n",
    "\t\t\tobs = env.reset()\n",
    "\t\t\timg = preprocess_observation(obs)\n",
    "\t\t\tstate = img\n",
    "\t\telse:\t\t\t\t\t\t\t\t\t# for the next 3 frames\n",
    "\t\t\tobs, _, _, _ = env.step(0)\n",
    "\t\t\timg = preprocess_observation(obs)\n",
    "\t\t\tstate = np.dstack((state, img))\n",
    "\treturn state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the evaluation at every 50k steps in order to monitor the performance during training \n",
    "def evaluation():\n",
    "\t# Reset all the return (real, discounted, clipped) to 0 for each time the evaluation is run\n",
    "\treal_return = clip_return = discounted_return = 0\n",
    "\n",
    "\t# run the evaluation for 100 episodes and take the average because each episodes is stochastics\n",
    "\tfor e in range(test_episode):\n",
    "\t\tprint(\"\\rTest: {} \".format(e), end=\"\")\n",
    "\n",
    "\t\t# Build up the initial 4 frames of observation\n",
    "\t\tevaluation_state = initial_4_frames()\n",
    "\n",
    "\t\t# reset the superscript of reward in each episodes \n",
    "\t\tsuperscript = 0\n",
    "\t\tndone = True\n",
    "\n",
    "\t\twhile ndone:\n",
    "\t\t\tenv.render()\n",
    "\t\t\t# select action based on the maximum value of action-value function (Q function)\n",
    "\t\t\tselected_action = np.argmax(Q_value.eval(feed_dict={X_state: [evaluation_state]})[0])\n",
    "\n",
    "\t\t\t# Step through the environment with the selected action\n",
    "\t\t\tobs, reward, done, _ = env.step(selected_action)\n",
    "\n",
    "\t\t\t# Store the previous observation for later calculation and update the 4 frames with the latest observation\n",
    "\t\t\told_obs = evaluation_state\n",
    "\t\t\tnext_state = preprocess_observation(obs)\n",
    "\t\t\tevaluation_state = np.delete(evaluation_state, [0], axis=2)\n",
    "\t\t\tevaluation_state = np.dstack((evaluation_state, next_state))\n",
    "\n",
    "\t\t\t# Evaluate the real, clip and discounted rewards\n",
    "\t\t\treal_return +=  reward\n",
    "\t\t\treward = np.clip(reward, -1, 1)\n",
    "\t\t\tclip_return += reward\n",
    "\t\t\tdiscounted_return += (discount_rate ** (superscript)) * reward\n",
    "\n",
    "\t\t\t# increase the superscript of the reward in each timestep \n",
    "\t\t\tsuperscript += 1\n",
    "\n",
    "\t\t\tif done:\n",
    "\t\t\t\tndone = False\n",
    "\n",
    "\t# calculate all the average rewards (real, clip and discounted) over 100 episodes as the indicators of the performance\n",
    "\taverage_real_return = real_return/test_episode\n",
    "\taverage_clip_return = clip_return/test_episode\n",
    "\taverage_discounted_return = discounted_return/test_episode\n",
    "\tprint ('Mean Real Return: %10f Mean Clip Return: %10f Mean Discounted Return: %10f' %(average_real_return, average_clip_return, average_discounted_return))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory if it not exist\n",
    "def check_directory(directory):\n",
    "\t#pathlib.Path(directory).mkdir(exist_ok=True) \n",
    "\tif not os.path.exists(directory):\n",
    "\t\tos.makedirs(directory)\n",
    "\n",
    "\n",
    "directory_final = './'+game+'/model_Final_'+game\n",
    "check_directory(directory_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./MsPacman/model_Final_MsPacman/MsPacman\n",
      "Test: 0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\fra_v\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\skimage\\transform\\_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "\tinit.run()\n",
    "\tsaver = tf.train.Saver()\n",
    "\tsaver.restore(sess, directory_final+'/'+game)\n",
    "\tevaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
